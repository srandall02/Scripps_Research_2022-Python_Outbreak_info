import datetime, os
from urllib.error import HTTPError
import pandas as pd
from epiweeks import Week
from arcgis.gis import GIS

# Download metadata from SEARCH repository
# https://raw.githubusercontent.com/andersen-lab/HCoV-19-Genomics/master/metadata.csv
def load_excite_providers() :
    excite = pd.read_csv( os.path.abspath('../resources/excite_providers.csv'), usecols=["ID", "provider" ] )
    excite = excite.set_index( "ID" )
    return excite["provider"].to_dict()

def load_file_as_list( loc ):
    with open( loc, "r" ) as open_file:
        return [line.strip() for line in open_file]

def download_search():
    """ Downloads the metadata from the SEARCH github repository. Removes entries with very wrong dates.
    Returns
    -------
    pandas.DataFrame:
        Data frame containing the metadata for all sequences generated by SEARCH
    """

    search_md = "https://raw.githubusercontent.com/andersen-lab/HCoV-19-Genomics/master/metadata.csv"
    md = pd.read_csv( search_md, usecols=["ID", "collection_date", "location", "authors", "originating_lab", "zipcode", "host", "percent_coverage_cds"] )
    md["collection_date"] = md["collection_date"].astype( str )

    # Filter out incorrect samples or wastewater
    md = md.loc[~md["ID"].isin(["SEARCH-104076", "SEARCH-58367"])]
    #md = md.loc[~md["ID"].isin( load_file_as_list( "resources/ignore.txt") )]

    md = md.loc[(md["location"]=="North America/USA/California/San Diego")|(md["location"].str.startswith( "North America/Mexico/Baja California" ))]

    md = md.loc[~md["collection_date"].str.startswith( "19" )]
    md = md.loc[~md["collection_date"].str.contains( "/" )]

    md = md.loc[~md["collection_date"].isin( ["NaT", "nan", 'Unknown', 'missing'] )]

    md = md.loc[~md["host"].isin(["Environment","Environmental"] )]

    # Generate an identifiable location column
    md["state"] = "Baja California"
    md.loc[md["location"]=="North America/USA/California/San Diego","state"] = "San Diego"

    #clean up zipcode
    md["zipcode"] = md["zipcode"].astype( "str" )
    md["zipcode"] = md["zipcode"].apply( lambda x: x.split( "-" )[0] )

    md["epiweek"] = md["collection_date"].apply( lambda x: Week.fromdate( datetime.datetime.strptime( x, "%Y-%m-%d" ).date() ).startdate() )
    md["collection_date"] = pd.to_datetime( md["collection_date"], format="%Y-%m-%d" ).dt.normalize()
    md["days_past"] = ( md["collection_date"].max() - md["collection_date"] ).dt.days

    md["originating_lab"] = md["originating_lab"].replace( { 'UC San Diego Center for Advanced Laboratory Medicine' :  "UCSD CALM Lab",
                                                            "UCSD EXCITE" : "UCSD EXCITE Lab",
                                                            "EXCITE Lab" : "UCSD EXCITE Lab",
                                                            "Andersen lab at Scripps Research" : "SD County Public Health Laboratory",
                                                            "San Diego County Public Health Laboratory" : "SD County Public Health Laboratory",
                                                            "Sharp HealthCare Laboratory" : "Sharp Health",
                                                            "Scripps Medical Laboratory" : "Scripps Health",
                                                            "Rady Children's Hospital - San Diego" : "Rady Children's Hospital",
                                                            "Rady Children’s Hospital" : "Rady Children's Hospital"} )

    excite_providers = load_excite_providers()

    # Correct some sequencer problems
    md["sequencer"] = "Andersen Lab"
    md.loc[md["originating_lab"]=="UCSD EXCITE Lab","sequencer"] = "UCSD EXCITE Lab"
    md.loc[md["authors"]=="Helix","sequencer"] = "Helix"
    md.loc[md["ID"].isin( load_file_as_list( os.path.abspath('../resources/sdphl_sequences.txt' )) ),"sequencer"] = "SD County Public Health Laboratory"

    md.loc[md['ID'].str.startswith( "CA-SDCPHL-" ),"sequencer"] = "SD County Public Health Laboratory"

    md["provider"] = md["originating_lab"]
    md.loc[md["originating_lab"]=="UCSD EXCITE Lab", "provider"] = md["ID"].map( excite_providers )
    md["provider"] = md["provider"].replace( {"RTL" : "UCSD Return to Learn",
                                             "CALM" : "UCSD CALM Lab",
                                             "HELIX" : "Helix",
                                             "San Diego Fire-Rescue Department" : "SD Fire-Rescue Department",
                                             "SASEA" : "UCSD Safer at School Early Action",
                                             "Instituto de Diagnostico y Referencia Epidemiologicos (InDRE)": "InDRE",
                                             "Delta" : "Helix",
                                             "DeltaAmplicon" : "Helix",
                                             "Genomica Lab Molecular, Mexico" : "Genomica Laboratorio",
                                             "Genomica Lab Molecular, México" : "Genomica Laboratorio"} )
    md.loc[md["provider"].isna(),"provider"] = md["sequencer"]

    # Add pangolin lineage information
    pango_loc = "https://raw.githubusercontent.com/andersen-lab/HCoV-19-Genomics/master/lineage_report.csv"
    pango = pd.read_csv( pango_loc, usecols=["taxon", "lineage"] )
    pango["num"] = pango["taxon"].str.extract( "SEARCH-([0-9]+)" )
    pango.loc[pango["num"].isna(),"num"] = pango["taxon"]
    pango = pango[["num", "lineage"]]


    md["num"] = md["ID"].str.extract( "SEARCH-([0-9]+)" )
    md.loc[md["num"].isna(),"num"] = md["ID"]


    md = md.merge( pango, left_on="num", right_on="num", how="left")

    # Filter sequences which failed lineage calling. These sequences are likely incomplete/erroneous.
    md = md.loc[~md["lineage"].isin( ["None", "Unassigned"] )]

    md = md[["ID","collection_date", "zipcode", "epiweek", "days_past", "sequencer", "provider", "lineage", "state"]]

    return md

def download_cases():
    """ Downloads the cases per San Diego ZIP code. Appends population.
    Returns
    -------
    pandas.DataFrame
        DataFrame detailing the cummulative cases in each ZIP code.
    """
    sd = download_sd_cases()
    bc = download_bc_cases()
    c = pd.concat( [sd,bc] )

    return c


def append_wastewater( sd ):
    zip_loc = "https://raw.githubusercontent.com/andersen-lab/SARS-CoV-2_WasteWater_San-Diego/master/Zipcodes.csv"
    zips = pd.read_csv( zip_loc, usecols=["Zip_code", "Wastewater_treatment_plant"], dtype={"Zip_code" : str, "Wastewater_treatment_plant" : str } )
    zips.columns = ["ziptext", "catchment"]
    zips["catchment"] = zips["catchment"].str.replace( " " , "" )
    zips = zips.set_index( "ziptext" )
    return_df = sd.merge( zips, left_on="ziptext", right_index=True, how="left" )
    return_df["catchment"] = return_df["catchment"].fillna( "Other" )

    assert return_df.shape[0] == sd.shape[0], f"Merge was unsuccessful. {sd.shape[0]} rows in original vs. {return_df.shape[0]} rows in merge output."
    return return_df

def download_sd_cases():
    """
    Returns
    -------
    pandas.DataFrame
        DataFrame detailing the daily number of cases in San Diego.
    """
    def _append_population( dataframe ):
        pop_loc = "resources/zip_pop.csv"
        pop = pd.read_csv( pop_loc, usecols=["Zip", "Total Population"], thousands=",", dtype={"Zip" : str, "Total Population" : int } )
        dataframe = dataframe.merge( pop, left_on="ziptext", right_on="Zip", how="left" )
        dataframe = dataframe.drop( columns=["Zip"] ).rename( columns={"Total Population" : "population"} )
        return dataframe

    def _add_missing_cases( entry ):
        entry = entry.set_index( "updatedate" ).reindex( pd.date_range( entry["updatedate"].min(), entry["updatedate"].max() ) ).rename_axis( "updatedate" ).reset_index()
        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=7)
        entry["new_cases"] = entry.rolling( window=indexer, min_periods=1 )["new_cases"].apply( lambda x: x.max() / 7 )
        return entry

    gis = GIS()
    cases_loc = "34b6df47e084441790813348c69d49ee"
    gis_layer = gis.content.get( cases_loc )
    features = gis_layer.layers[0].query( where='1=1' )
    sd = features.df
    sd = sd[["ziptext","case_count", "updatedate"]]
    sd["updatedate"] = pd.to_datetime( sd["updatedate"] ).dt.tz_localize( None )
    sd["updatedate"] = sd["updatedate"].dt.normalize()
    #sd["ziptext"] = pd.to_numeric( sd["ziptext"] )
    sd = sd.groupby( ["updatedate", "ziptext"] ).last().reset_index()
    sd = sd.sort_values( "updatedate" )

    # Calculate cases per day because that's way more usable than cumulative counts.
    sd["case_count"] = sd["case_count"].fillna( 0 )
    sd["new_cases"] = sd.groupby( "ziptext" )["case_count"].diff()
    sd["new_cases"] = sd["new_cases"].fillna( sd["case_count"] )
    sd.loc[sd["new_cases"]<0, "new_cases"] = 0

    # Brief hack because SD stopped reporting daily cases and instead reports weekly cases after 2021-06-29.
    sdprob = sd.loc[sd["updatedate"]>"2021-06-28"]
    sdprob = sdprob.groupby( "ziptext" ).apply( _add_missing_cases )
    sdprob = sdprob.drop( columns="ziptext" ).reset_index()
    sdprob = sdprob.drop( columns="level_1" )

    sd = sd.loc[sd["updatedate"]<="2021-06-28"]
    sd = pd.concat( [sd,sdprob] )

    sd = _append_population( sd )

    sd["days_past"] = ( datetime.datetime.today() - sd["updatedate"] ).dt.days

    sd["case_count"] = sd.groupby( "ziptext" )["new_cases"].cumsum()

    # Add the catchment area
    sd = append_wastewater( sd )
    return sd

def download_bc_cases():
    """
    Returns
    -------
    pandas.DataFrame
        DateFrame detailing the daily number of cases in Baja California, Mexico
    """
    today = datetime.datetime.today()
    date_range = 10
    attempts = 0
    while attempts < date_range:
        print( f"Attemping to load BC data from {today.strftime( '%Y-%m-%d')}" )
        date_url = int( today.strftime( "%Y%m%d" ) )
        bc_url = f"https://datos.covid-19.conacyt.mx/Downloads/Files/Casos_Diarios_Estado_Nacional_Confirmados_{date_url}.csv"

        try:
            bc = pd.read_csv( bc_url, index_col="nombre" )
            break
        except HTTPError:
            today = today - datetime.timedelta( days=1 )
    else:
        raise RuntimeError( f"Unable to find a valid download link. Last url tried was {bc_url}" )

    bc = bc.drop( columns=["cve_ent", "poblacion"] )
    bc = bc.T
    bc = bc["BAJA CALIFORNIA"].reset_index()
    bc["index"] = pd.to_datetime( bc["index"], format="%d-%m-%Y" ).dt.tz_localize( None )
    bc["index"] = bc["index"].dt.normalize()
    bc.columns = ["updatedate", "new_cases"]
    bc = bc.sort_values( "updatedate" )

    # Generate the additional columns
    bc["case_count"] = bc["new_cases"].cumsum()
    bc["ziptext"] = "None"
    bc["population"] = 3648100
    bc["days_past"] = ( today - bc["updatedate"] ).dt.days

    bc = bc.loc[bc["case_count"] > 0]

    return bc

if __name__ == "__main__":
    seqs_md = download_search()
    seqs_md.to_csv( os.path.abspath('../resources/sequences.csv'), index=False )

    cases = download_cases()
    cases.to_csv( os.path.abspath('../resources/cases.csv'), index=False )
